{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "373e8710-3bad-444b-9e8b-4e03e53ab7d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MSc Cyber Security Project — Pratham Shrestha\n",
    "# Notebook 02 — Preprocessing \n",
    "\n",
    "# --- Imports and setup ---\n",
    "# importing all the main libraries I need for data preparation\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import joblib\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from imblearn.over_sampling import SMOTE  # for balancing attack/benign classes\n",
    "\n",
    "# using one constant random seed everywhere for reproducibility\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "# folder setup: same logic as before to make sure paths work in any environment\n",
    "BASE = Path(\"..\") if Path.cwd().name == \"notebooks\" else Path(\".\")\n",
    "RAW_CSV = BASE / \"dataset\" / \"raw\" / \"ALLFLOWMETER_HIKARI2021.csv\"\n",
    "PROC_DIR = BASE / \"dataset\" / \"processed\"\n",
    "PROC_DIR.mkdir(parents=True, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "35ac3190-544d-4fe0-a5c3-7ac4a65d57a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw dataset shape: (555278, 88)\n"
     ]
    }
   ],
   "source": [
    "# --- Load the dataset ---\n",
    "# loading the HIKARI-2021 dataset\n",
    "df = pd.read_csv(RAW_CSV, low_memory=False)\n",
    "print(\"Raw dataset shape:\", df.shape)\n",
    "\n",
    "# making sure Label column is available (0=normal, 1=attack)\n",
    "assert \"Label\" in df.columns, \"Expected a 'Label' column in the dataset.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4bccea4d-1576-4f9f-b2dd-2a46635c73bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropped columns: ['traffic_category', 'uid', 'originh', 'responh', 'Unnamed: 0', 'Unnamed: 0.1']\n"
     ]
    }
   ],
   "source": [
    "# --- Drop leakage and irrelevant columns ---\n",
    "# removing columns that either leak label info or don't contribute to model learning\n",
    "LEAKY_OR_ID = [\n",
    "    \"traffic_category\",       # duplicates the label info (causes leakage)\n",
    "    \"attack_cat\", \"attack_category\", \"label_name\",\n",
    "    \"LabelNum\", \"label_num\",  # other variations of the same label\n",
    "    \"uid\", \"originh\", \"responh\",  # identifiers or hostnames (high cardinality)\n",
    "    \"Unnamed: 0\", \"Unnamed: 0.1\"  # leftover index columns\n",
    "]\n",
    "\n",
    "# dropping only if they exist in the dataframe\n",
    "to_drop = [c for c in LEAKY_OR_ID if c in df.columns]\n",
    "df = df.drop(columns=to_drop) if to_drop else df\n",
    "print(\"Dropped columns:\", to_drop if to_drop else \"None\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7c7cb1ea-d1c0-41f0-a077-3c598c90c3e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed 36358 duplicate rows. New size: 518920\n"
     ]
    }
   ],
   "source": [
    "# --- Remove duplicate rows ---\n",
    "# this prevents data leakage between training and test splits\n",
    "before = len(df)\n",
    "df = df.drop_duplicates()\n",
    "print(f\"Removed {before - len(df)} duplicate rows. New size: {len(df)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f9d1ce9d-0c3c-418a-ae91-67c71f152493",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kept 81 numeric features out of 81 total columns.\n"
     ]
    }
   ],
   "source": [
    "# --- Separate features and labels ---\n",
    "# assigning 'Label' as target (y) and rest as features (X)\n",
    "y = df[\"Label\"].astype(int)\n",
    "X_all = df.drop(columns=[\"Label\"])\n",
    "\n",
    "# selecting only numeric features to simplify preprocessing\n",
    "num_cols = list(X_all.select_dtypes(include=[np.number]).columns)\n",
    "X = X_all[num_cols].copy()\n",
    "\n",
    "print(f\"Kept {len(num_cols)} numeric features out of {X_all.shape[1]} total columns.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0e15adb8-73c2-4915-8701-a516f6a06556",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (415136, 81)\n",
      "Test shape : (103784, 81)\n",
      "\n",
      "Training class balance (%):\n",
      " Label\n",
      "0    92.77\n",
      "1     7.23\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "Testing class balance (%):\n",
      " Label\n",
      "0    92.77\n",
      "1     7.23\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# --- Train-test split ---\n",
    "# stratified split keeps the same class ratio in both sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.20, random_state=RANDOM_STATE, stratify=y\n",
    ")\n",
    "\n",
    "print(\"Train shape:\", X_train.shape)\n",
    "print(\"Test shape :\", X_test.shape)\n",
    "print(\"\\nTraining class balance (%):\\n\", (y_train.value_counts(normalize=True) * 100).round(2))\n",
    "print(\"\\nTesting class balance (%):\\n\", (y_test.value_counts(normalize=True) * 100).round(2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "937d175a-17db-4dfa-b11c-30404b3b63a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After preprocessing: (415136, 81) (103784, 81)\n"
     ]
    }
   ],
   "source": [
    "# --- Scaling and imputing numeric features ---\n",
    "# simple preprocessing: fill missing values with median and scale values\n",
    "preprocessor = Pipeline(steps=[\n",
    "    (\"impute\", SimpleImputer(strategy=\"median\")),\n",
    "    (\"scale\", StandardScaler())\n",
    "])\n",
    "\n",
    "# fitting only on training data to prevent data leakage\n",
    "preprocessor.fit(X_train)\n",
    "\n",
    "# transforming both train and test sets using the same fitted preprocessor\n",
    "X_train_proc = preprocessor.transform(X_train)\n",
    "X_test_proc = preprocessor.transform(X_test)\n",
    "\n",
    "print(\"After preprocessing:\", X_train_proc.shape, X_test_proc.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ef44ed84-ea77-46c1-b5db-dbb2a78463cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Balanced training shape: (550162, 81)\n",
      "\n",
      "Class distribution after SMOTE (train):\n",
      "        Count  Percentage\n",
      "Label                    \n",
      "0      385114        70.0\n",
      "1      165048        30.0\n"
     ]
    }
   ],
   "source": [
    "# --- Balancing with SMOTE (on training set only) ---\n",
    "# applying SMOTE moderately to reach about 70:30 ratio of benign:attack\n",
    "TARGET_RATIO = 0.30 / 0.70  # ≈ 0.43 minority/majority ratio\n",
    "\n",
    "smote = SMOTE(sampling_strategy=TARGET_RATIO, random_state=RANDOM_STATE)\n",
    "X_train_bal, y_train_bal = smote.fit_resample(X_train_proc, y_train)\n",
    "\n",
    "print(\"Balanced training shape:\", X_train_bal.shape)\n",
    "print(\"\\nClass distribution after SMOTE (train):\")\n",
    "\n",
    "# show both counts and percentage\n",
    "value_counts = pd.Series(y_train_bal).value_counts()\n",
    "percentages = (value_counts / len(y_train_bal) * 100).round(2)\n",
    "\n",
    "summary_df = pd.DataFrame({\n",
    "    \"Count\": value_counts,\n",
    "    \"Percentage\": percentages\n",
    "})\n",
    "\n",
    "print(summary_df)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4d272dde-19f8-46c1-a75e-39e2756af006",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved all processed files to: ..\\dataset\\processed\n"
     ]
    }
   ],
   "source": [
    "# --- Save processed data and objects ---\n",
    "# saving arrays, labels, and fitted preprocessor for next notebook\n",
    "np.save(PROC_DIR / \"X_train_bal_70_30.npy\", X_train_bal)\n",
    "np.save(PROC_DIR / \"X_test_proc.npy\", X_test_proc)\n",
    "\n",
    "pd.Series(y_train_bal, name=\"Label\").to_csv(PROC_DIR / \"y_train_bal_70_30.csv\", index=False)\n",
    "pd.Series(y_test, name=\"Label\").to_csv(PROC_DIR / \"y_test.csv\", index=False)\n",
    "\n",
    "joblib.dump(preprocessor, PROC_DIR / \"preprocessor.joblib\")\n",
    "\n",
    "print(\"Saved all processed files to:\", PROC_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "886b1fd6-f6ba-48fa-961e-2bdeadb60dac",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:Anacondo]",
   "language": "python",
   "name": "conda-env-Anacondo-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
